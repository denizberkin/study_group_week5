{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoping at this point, we are familiar with classification, object detection can be explained as a classification with localization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "![mater](assets/mcqueen_real.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Localization (and over multiple objects)\n",
    "\n",
    "![mcqueen](assets/mcqueen.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Could be used on any kind of task where finding the location of the object(s) are of any use\n",
    "- Anything related to traffic, pedestrians, types of vehicles, drivable roads, landing zones etc.\n",
    "- Anything related to locating a disease over some type of medical imaging (MRI, Ultrasound, CT ...)\n",
    "- When designing automated stores, factories etc. (Like Amazon Go cashierless stores)\n",
    "\n",
    "this could go on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yea yea yea its all good but how does it come to be and how can I learn / use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay we are kind of familiar with a CNN, it acts as a feature extractor, connects to a FCN with number of classes as neurons for output and ta-dah, we have a multi class classifier. \n",
    "\n",
    "![out_neurons](assets/detection_output_neurons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![annotated_out_neurons](assets/annotated_output_neurons.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be penalized with any loss but main logic here is that \n",
    "loss_fn = lambda x, y: (x - y) ** 2\n",
    "prediction = [1] * 8  # P, x, y, w, h, c1, c2, c3\n",
    "label = [1] * 8\n",
    "\n",
    "if prediction[0]:\n",
    "    # calculate loss for only first neuron, we want it to be 0\n",
    "    loss = loss_fn(prediction[0], label[0])\n",
    "else:\n",
    "    # calculate loss over all the other predictions as well\n",
    "    loss = sum([loss_fn(p, l) for p, l in zip(prediction, label)])  # you do not have to use one type of loss function here\n",
    "    # you can use variation of losses which may differ from a bounding box to a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But how do we classify an unknown number of objects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let me explain while expanding on some utility functions that make object detection the way it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding window detection\n",
    "\n",
    "Sliding window detection is like searching for your car in a crowded parking lot - \n",
    "except instead of cars, it is any kind of object in the image. This method involves repeatedly \n",
    "applying the same feature detector or \"window\" to an image at multiple locations \n",
    "and scales. As it slides around, it checks each spot to see if there is a good match.\n",
    "\n",
    "**Example:** Imagine you're looking for your snail hiding behind you in the house. A sliding window detector would move its feature template over the \n",
    "image, checking possible locations where your cat might be hiding, at different \n",
    "scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/sliding_snail.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Iterable\n",
    "import numpy as np\n",
    "\n",
    "def sliding_window(image: np.array, step_size: int, window_size: Tuple[int, int]) -> Iterable[int, int, np.ndarray]:\n",
    "    H, W = image.shape  # considering image is 2 channels, you should put a check here\n",
    "    for y in range(0, H, step_size):\n",
    "        for x in range(0, W, step_size):\n",
    "            yield (x, y, image[y: y + window_size[1], x: x + window_size[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding Windows over Convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![swin_conv](assets/sliding_window_conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image reference](https://www.coursera.org/learn/convolutional-neural-networks/lecture/6UnU4/convolutional-implementation-of-sliding-windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersection Over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoU is a measure of how well two objects that cover an area fit together, in this case the prediction and the ground truth. \n",
    "\n",
    "IoU, stated by its name as well, simply calculates the ratio of the intersection area to the union area between two \n",
    "bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/iou-formula.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/iou-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image_1 reference](https://idiotdeveloper.com/what-is-intersection-over-union-iou/)\n",
    "\n",
    "[image_2 reference](https://www.superannotate.com/blog/intersection-over-union-for-object-detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(pred: Tuple[int, int, int, int], \n",
    "        gt: Tuple[int, int, int, int]) -> float:\n",
    "    \"\"\" in xyxy format, you can write it as xywh format if you'd like \"\"\"\n",
    "    # intersection points\n",
    "    x1 = max(pred[0], gt[0])\n",
    "    y1 = max(pred[0], gt[0])\n",
    "    x2 = max(pred[0], gt[0])\n",
    "    y2 = max(pred[0], gt[0])\n",
    "\n",
    "    # intersection\n",
    "    intersection = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "\n",
    "    # area of boxes\n",
    "    area_pred = (pred[2] - pred[0] + 1) * (pred[3] - pred[1] + 1)\n",
    "    area_gt = (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1)\n",
    "\n",
    "    iou = intersection / float(area_pred + area_gt - intersection)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anchor Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anchor boxes are like the buffet of object detection - they offer multiple choices or \"anchors\" for bounding box predictions. Instead of predicting a single box, an anchor box-based detector proposes a range of possible boxes that might contain an object.\n",
    "\n",
    "**Example:** Imagine you are trying to detect all the animals in an image. An anchor box-based detector would propose multiple bounding boxes with different sizes and aspect ratios, covering possible locations and orientations of the animals. The algorithm then adjusts these anchors based on the detected objects characteristics, like size and shape, to get a more accurate detection result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![anchor](assets/anchor_box.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_boxes(scales: list, aspect_ratios: list, image_size: Tuple[int, int]):\n",
    "    anchor_boxes = []\n",
    "    for scale in scales:  # different sizes for anchor boxes\n",
    "        for ratio in aspect_ratios:\n",
    "            width = scale * np.sqrt(ratio)\n",
    "            height = scale / np.sqrt(ratio)\n",
    "            # create anchor box\n",
    "            anchor_boxes.append([width, height])\n",
    "    return anchor_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Max Suppression (NMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that grid cells are used in anchor boxes and will learn different shapes and orientations. But running the algorithm, you will see that there are many unnecessary detections (can be observed in the image below). \n",
    "\n",
    "Non-max suppression is by name, an algorithm that supresses the bounding boxes of the same grid with lower than certain threshold and iou value with respect to the other bounding boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/nms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image reference](https://learnopencv.com/weighted-boxes-fusion/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_supression(boxes: list, scores: list, threshold: float = 0.5):\n",
    "    if len(boxes) == 0:\n",
    "        return []  # no prediction to supress\n",
    "    \n",
    "    # it is good to work with np arrays / easier if it is not already that way\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    # sorting bbox confidence scores in descending order\n",
    "    indices = np.argsort(scores)[::-1]\n",
    "    picked = []\n",
    "\n",
    "    while len(indices) > 0:\n",
    "        current = indices[0]\n",
    "        picked.append(current)\n",
    "\n",
    "        # compute iou for all of the rest\n",
    "        remaining = indices[1:]\n",
    "        ious = np.array([iou(boxes[current], boxes[i]) for i in remaining])\n",
    "\n",
    "        indices = remaining[ious < threshold]  # elliminate boxes that computes iou less than the threshold\n",
    "\n",
    "    return boxes[picked]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side note on how YOLO calculates loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(maybe not the current ones like YOLO7-8-9-10..., can't keep track of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_loss_fn, bce, categorical_ce = None, None, None\n",
    "\n",
    "def yolo_loss(predictions, ground_truth, anchors):\n",
    "    # Split predictions into components\n",
    "    obj_preds = predictions[..., 0]   # objectness\n",
    "    box_preds = predictions[..., 1:5]  # x, y, w, h\n",
    "    class_preds = predictions[..., 6:] # class predictions\n",
    "    \n",
    "    # \"is there\" an object?\n",
    "    obj_loss = bce(obj_preds, ground_truth[..., 0])\n",
    "    \n",
    "    # \"how much\" of the object we have correctly guessed\n",
    "    iou_loss = iou_loss_fn(box_preds, ground_truth[..., 1:5])\n",
    "    \n",
    "    # did we guess \"which\" object it is\n",
    "    class_loss = categorical_ce(class_preds, ground_truth[..., 6:])\n",
    "    \n",
    "    return iou_loss + obj_loss + class_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What YOLO does other than that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image reference](assets/yolo_dls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image reference](https://www.coursera.org/learn/convolutional-neural-networks/lecture/fF3O0/yolo-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some limitations of original yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Struggles to generalize objects that does not fit the anchor boxes, different aspect ratio objects\n",
    "- Struggles to differentiate between small errors on large boxes vs same errors in smaller boxes are huge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's just infer stuff with yolo for fun, yolov5 is in torch.hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/denizberkin/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
      "View Ultralytics Settings with 'yolo settings' or at '/home/denizberkin/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2024-10-5 Python-3.11.9 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:00<00:00, 48.4MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "/home/denizberkin/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "image 1/5: 720x1280 2 persons, 2 ties\n",
      "image 2/5: 880x1580 2 cars, 1 boat\n",
      "image 3/5: 1080x1920 1 person\n",
      "image 4/5: 3116x4816 1 sports ball, 1 fork, 1 knife, 1 apple, 1 scissors\n",
      "image 5/5: 1080x1920 1 person, 1 chair, 2 tvs, 1 refrigerator\n",
      "Speed: 917.6ms pre-process, 15.2ms inference, 35.1ms NMS per image at shape (5, 3, 416, 640)\n",
      "Saved 5 images to \u001b[1mruns/detect/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# pred stuff on yolo\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Images\n",
    "imgs = [\n",
    "    \"https://ultralytics.com/images/zidane.jpg\",\n",
    "    \n",
    "    \"https://lumiere-a.akamaihd.net/v1/images/open-uri20150608-27674-iuiafs_2fd2629d.jpeg\",\n",
    "\n",
    "    \"https://wallpapercave.com/wp/s1o8rpn.jpg\",\n",
    "\n",
    "    \"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.pinimg.com%2Foriginals%2F36%2Fcd%2Feb%2F36cdebcd4fdd7eef3c9d0723cb0a886e.jpg\",\n",
    "\n",
    "    \"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.cctvcamerapros.com%2Fv%2Fimages%2FHD-Security-Cameras%2FHD-TVI-BL2%2Finfrared-HD-TVI-camera-1080p-surveillance.jpg\",\n",
    "    ]\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "\n",
    "# Results\n",
    "results.print()\n",
    "results.save()  # or .show()\n",
    "# results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inzva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
