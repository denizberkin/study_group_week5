{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoping at this point, we are familiar with classification, object detection can be explained as a classification with localization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "![mater](assets/mcqueen_real.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Localization (and over multiple objects)\n",
    "\n",
    "![mcqueen](assets/mcqueen.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Could be used on any kind of task where finding the location of the object(s) are of any use\n",
    "- Anything related to traffic, pedestrians, types of vehicles, drivable roads, landing zones etc.\n",
    "- Anything related to locating a disease over some type of medical imaging (MRI, Ultrasound, CT ...)\n",
    "- When designing automated stores, factories etc. (Like Amazon Go cashierless stores)\n",
    "\n",
    "this could go on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yea yea yea its all good but how does it come to be and how can I learn / use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay we are kind of familiar with a CNN, it acts as a feature extractor, connects to a FCN with number of classes as neurons for output and ta-dah, we have a multi class classifier. \n",
    "\n",
    "![out_neurons](assets/detection_output_neurons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![annotated_out_neurons](assets/annotated_output_neurons.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be penalized with any loss but main logic here is that \n",
    "loss_fn = lambda x, y: (x - y) ** 2\n",
    "prediction = [1] * 8  # P, x, y, w, h, c1, c2, c3\n",
    "label = [1] * 8\n",
    "\n",
    "if prediction[0]:\n",
    "    # calculate loss for only first neuron, we want it to be 0\n",
    "    loss = loss_fn(prediction[0], label[0])\n",
    "else:\n",
    "    # calculate loss over all the other predictions as well\n",
    "    loss = sum([loss_fn(p, l) for p, l in zip(prediction, label)])  # you do not have to use one type of loss function here\n",
    "    # you can use variation of losses which may differ from a bounding box to a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But how do we classify an unknown number of objects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let me explain while expanding on some utility functions that make object detection the way it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding window detection\n",
    "\n",
    "Sliding window detection is like searching for your car in a crowded parking lot - \n",
    "except instead of cars, it is any kind of object in the image. This method involves repeatedly \n",
    "applying the same feature detector or \"window\" to an image at multiple locations \n",
    "and scales. As it slides around, it checks each spot to see if there is a good match.\n",
    "\n",
    "**Example:** Imagine you're looking for your snail hiding behind you in the house. A sliding window detector would move its feature template over the \n",
    "image, checking possible locations where your cat might be hiding, at different \n",
    "scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/sliding_snail.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Iterable\n",
    "import numpy as np\n",
    "\n",
    "def sliding_window(image: np.array, step_size: int, window_size: Tuple[int, int]) -> Iterable[int, int, np.ndarray]:\n",
    "    H, W = image.shape  # considering image is 2 channels, you should put a check here\n",
    "    for y in range(0, H, step_size):\n",
    "        for x in range(0, W, step_size):\n",
    "            yield (x, y, image[y: y + window_size[1], x: x + window_size[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding Windows over Convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![swin_conv](assets/sliding_window_conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image reference](https://www.coursera.org/learn/convolutional-neural-networks/lecture/6UnU4/convolutional-implementation-of-sliding-windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersection Over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoU is a measure of how well two objects that cover an area fit together, in this case the prediction and the ground truth. \n",
    "\n",
    "IoU, stated by its name as well, simply calculates the ratio of the intersection area to the union area between two \n",
    "bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/iou-formula.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/iou-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image_1 reference](https://idiotdeveloper.com/what-is-intersection-over-union-iou/)\n",
    "\n",
    "[image_2 reference](https://www.superannotate.com/blog/intersection-over-union-for-object-detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(pred: Tuple[int, int, int, int], \n",
    "        gt: Tuple[int, int, int, int]) -> float:\n",
    "    \"\"\" in xyxy format, you can write it as xywh format if you'd like \"\"\"\n",
    "    # intersection points\n",
    "    x1 = max(pred[0], gt[0])\n",
    "    y1 = max(pred[0], gt[0])\n",
    "    x2 = max(pred[0], gt[0])\n",
    "    y2 = max(pred[0], gt[0])\n",
    "\n",
    "    # intersection\n",
    "    intersection = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "\n",
    "    # area of boxes\n",
    "    area_pred = (pred[2] - pred[0] + 1) * (pred[3] - pred[1] + 1)\n",
    "    area_gt = (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1)\n",
    "\n",
    "    iou = intersection / float(area_pred + area_gt - intersection)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anchor Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anchor boxes are like the buffet of object detection - they offer multiple choices or \"anchors\" for bounding box predictions. Instead of predicting a single box, an anchor box-based detector proposes a range of possible boxes that might contain an object.\n",
    "\n",
    "**Example:** Imagine you are trying to detect all the animals in an image. An anchor box-based detector would propose multiple bounding boxes with different sizes and aspect ratios, covering possible locations and orientations of the animals. The algorithm then adjusts these anchors based on the detected objects characteristics, like size and shape, to get a more accurate detection result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![anchor](assets/anchor_box.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_boxes(scales: list, aspect_ratios: list, image_size: Tuple[int, int]):\n",
    "    anchor_boxes = []\n",
    "    for scale in scales:  # different sizes for anchor boxes\n",
    "        for ratio in aspect_ratios:\n",
    "            width = scale * np.sqrt(ratio)\n",
    "            height = scale / np.sqrt(ratio)\n",
    "            # create anchor box\n",
    "            anchor_boxes.append([width, height])\n",
    "    return anchor_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Max Suppression (NMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that grid cells are used in anchor boxes and will learn different shapes and orientations. But running the algorithm, you will see that there are many unnecessary detections (can be observed in the image below). \n",
    "\n",
    "Non-max suppression is by name, an algorithm that supresses the bounding boxes of the same grid with lower than certain threshold and iou value with respect to the other bounding boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/nms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image reference](https://learnopencv.com/weighted-boxes-fusion/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_supression(boxes: list, scores: list, threshold: float = 0.5):\n",
    "    if len(boxes) == 0:\n",
    "        return []  # no prediction to supress\n",
    "    \n",
    "    # it is good to work with np arrays / easier if it is not already that way\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    # sorting bbox confidence scores in descending order\n",
    "    indices = np.argsort(scores)[::-1]\n",
    "    picked = []\n",
    "\n",
    "    while len(indices) > 0:\n",
    "        current = indices[0]\n",
    "        picked.append(current)\n",
    "\n",
    "        # compute iou for all of the rest\n",
    "        remaining = indices[1:]\n",
    "        ious = np.array([iou(boxes[current], boxes[i]) for i in remaining])\n",
    "\n",
    "        indices = remaining[ious < threshold]  # elliminate boxes that computes iou less than the threshold\n",
    "\n",
    "    return boxes[picked]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side note on how YOLO calculates loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(maybe not the current ones like YOLO7-8-9-10..., can't keep track of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_loss_fn, bce, categorical_ce = None, None, None\n",
    "\n",
    "def yolo_loss(predictions, ground_truth, anchors):\n",
    "    # Split predictions into components\n",
    "    obj_preds = predictions[..., 0]   # objectness\n",
    "    box_preds = predictions[..., 1:5]  # x, y, w, h\n",
    "    class_preds = predictions[..., 6:] # class predictions\n",
    "    \n",
    "    # \"is there\" an object?\n",
    "    obj_loss = bce(obj_preds, ground_truth[..., 0])\n",
    "    \n",
    "    # \"how much\" of the object we have correctly guessed\n",
    "    iou_loss = iou_loss_fn(box_preds, ground_truth[..., 1:5])\n",
    "    \n",
    "    # did we guess \"which\" object it is\n",
    "    class_loss = categorical_ce(class_preds, ground_truth[..., 6:])\n",
    "    \n",
    "    return iou_loss + obj_loss + class_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What YOLO does other than that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image reference](assets/yolo_dls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image reference](https://www.coursera.org/learn/convolutional-neural-networks/lecture/fF3O0/yolo-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's just infer stuff with yolo for fun, yolov5 is in torch.hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denizberkin/miniconda3/envs/inzva/lib/python3.11/site-packages/torch/hub.py:295: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /home/denizberkin/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# pred stuff on yolo\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43multralytics/yolov5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov5s\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Images\u001b[39;00m\n\u001b[1;32m      8\u001b[0m imgs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://ultralytics.com/images/zidane.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://external-content.duckduckgo.com/iu/?u=https\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m3A\u001b[39m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;124mwww.cctvcamerapros.com\u001b[39m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;124mv\u001b[39m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;124mHD-Security-Cameras\u001b[39m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;124mHD-TVI-BL2\u001b[39m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;124minfrared-HD-TVI-camera-1080p-surveillance.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/torch/hub.py:570\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    567\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    568\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 570\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/torch/hub.py:599\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[1;32m    598\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m--> 599\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:215\u001b[0m, in \u001b[0;36myolov5s\u001b[0;34m(pretrained, channels, classes, autoshape, _verbose, device)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myolov5s\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, autoshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    Create a YOLOv5-small (yolov5s) model with options for pretraining, input channels, class count, autoshaping,\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    verbosity, and device configuration.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m        the [YOLOv5 PyTorch Hub Documentation](https://pytorch.org/hub/ultralytics_yolov5).\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolov5s\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:54\u001b[0m, in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mCreates or loads a YOLOv5 model, with options for pretrained weights and model customization.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    [YOLOv5 PyTorch Hub Documentation](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading).\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoShape, DetectMultiBackend\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attempt_load\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassificationModel, DetectionModel, SegmentationModel\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Import 'ultralytics' package or install if missing\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ultralytics, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# verify package is not directory\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/ultralytics/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Set ENV Variables (place before imports)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOMP_NUM_THREADS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# reduce CPU utilization during training\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplorer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Explorer\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NAS, RTDETR, SAM, YOLO, FastSAM, YOLOWorld\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ASSETS, SETTINGS\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/ultralytics/data/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ultralytics YOLO 🚀, AGPL-3.0 license\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseDataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_dataloader, build_grounding, build_yolo_dataset, load_inference_source\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     ClassificationDataset,\n\u001b[1;32m      7\u001b[0m     GroundingDataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     YOLOMultiModalDataset,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/ultralytics/data/base.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsutil\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FORMATS_HELP_MSG, HELP_URL, IMG_FORMATS\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_CFG, LOCAL_RANK, LOGGER, NUM_THREADS, TQDM\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseDataset\u001b[39;00m(Dataset):\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/ultralytics/data/utils.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageOps\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautobackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_class_names\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     DATASETS_DIR,\n\u001b[1;32m     22\u001b[0m     LOGGER,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     yaml_save,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchecks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_file, check_font, is_ascii\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/ultralytics/nn/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ultralytics YOLO 🚀, AGPL-3.0 license\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     BaseModel,\n\u001b[1;32m      5\u001b[0m     ClassificationModel,\n\u001b[1;32m      6\u001b[0m     DetectionModel,\n\u001b[1;32m      7\u001b[0m     SegmentationModel,\n\u001b[1;32m      8\u001b[0m     attempt_load_one_weight,\n\u001b[1;32m      9\u001b[0m     attempt_load_weights,\n\u001b[1;32m     10\u001b[0m     guess_model_scale,\n\u001b[1;32m     11\u001b[0m     guess_model_task,\n\u001b[1;32m     12\u001b[0m     parse_model,\n\u001b[1;32m     13\u001b[0m     torch_safe_load,\n\u001b[1;32m     14\u001b[0m     yaml_model_load,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattempt_load_one_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattempt_load_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseModel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/ultralytics/nn/tasks.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     AIFI,\n\u001b[1;32m     14\u001b[0m     C1,\n\u001b[1;32m     15\u001b[0m     C2,\n\u001b[1;32m     16\u001b[0m     C2PSA,\n\u001b[1;32m     17\u001b[0m     C3,\n\u001b[1;32m     18\u001b[0m     C3TR,\n\u001b[1;32m     19\u001b[0m     ELAN1,\n\u001b[1;32m     20\u001b[0m     OBB,\n\u001b[1;32m     21\u001b[0m     PSA,\n\u001b[1;32m     22\u001b[0m     SPP,\n\u001b[1;32m     23\u001b[0m     SPPELAN,\n\u001b[1;32m     24\u001b[0m     SPPF,\n\u001b[1;32m     25\u001b[0m     AConv,\n\u001b[1;32m     26\u001b[0m     ADown,\n\u001b[1;32m     27\u001b[0m     Bottleneck,\n\u001b[1;32m     28\u001b[0m     BottleneckCSP,\n\u001b[1;32m     29\u001b[0m     C2f,\n\u001b[1;32m     30\u001b[0m     C2fAttn,\n\u001b[1;32m     31\u001b[0m     C2fCIB,\n\u001b[1;32m     32\u001b[0m     C2fPSA,\n\u001b[1;32m     33\u001b[0m     C3Ghost,\n\u001b[1;32m     34\u001b[0m     C3k2,\n\u001b[1;32m     35\u001b[0m     C3x,\n\u001b[1;32m     36\u001b[0m     CBFuse,\n\u001b[1;32m     37\u001b[0m     CBLinear,\n\u001b[1;32m     38\u001b[0m     Classify,\n\u001b[1;32m     39\u001b[0m     Concat,\n\u001b[1;32m     40\u001b[0m     Conv,\n\u001b[1;32m     41\u001b[0m     Conv2,\n\u001b[1;32m     42\u001b[0m     ConvTranspose,\n\u001b[1;32m     43\u001b[0m     Detect,\n\u001b[1;32m     44\u001b[0m     DWConv,\n\u001b[1;32m     45\u001b[0m     DWConvTranspose2d,\n\u001b[1;32m     46\u001b[0m     Focus,\n\u001b[1;32m     47\u001b[0m     GhostBottleneck,\n\u001b[1;32m     48\u001b[0m     GhostConv,\n\u001b[1;32m     49\u001b[0m     HGBlock,\n\u001b[1;32m     50\u001b[0m     HGStem,\n\u001b[1;32m     51\u001b[0m     ImagePoolingAttn,\n\u001b[1;32m     52\u001b[0m     Pose,\n\u001b[1;32m     53\u001b[0m     RepC3,\n\u001b[1;32m     54\u001b[0m     RepConv,\n\u001b[1;32m     55\u001b[0m     RepNCSPELAN4,\n\u001b[1;32m     56\u001b[0m     RepVGGDW,\n\u001b[1;32m     57\u001b[0m     ResNetLayer,\n\u001b[1;32m     58\u001b[0m     RTDETRDecoder,\n\u001b[1;32m     59\u001b[0m     SCDown,\n\u001b[1;32m     60\u001b[0m     Segment,\n\u001b[1;32m     61\u001b[0m     WorldDetect,\n\u001b[1;32m     62\u001b[0m     v10Detect,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS, LOGGER, colorstr, emojis, yaml_load\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchecks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_requirements, check_suffix, check_yaml\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/ultralytics/nn/modules/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ultralytics YOLO 🚀, AGPL-3.0 license\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mUltralytics modules.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblock\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     C1,\n\u001b[1;32m     22\u001b[0m     C2,\n\u001b[1;32m     23\u001b[0m     C2PSA,\n\u001b[1;32m     24\u001b[0m     C3,\n\u001b[1;32m     25\u001b[0m     C3TR,\n\u001b[1;32m     26\u001b[0m     CIB,\n\u001b[1;32m     27\u001b[0m     DFL,\n\u001b[1;32m     28\u001b[0m     ELAN1,\n\u001b[1;32m     29\u001b[0m     PSA,\n\u001b[1;32m     30\u001b[0m     SPP,\n\u001b[1;32m     31\u001b[0m     SPPELAN,\n\u001b[1;32m     32\u001b[0m     SPPF,\n\u001b[1;32m     33\u001b[0m     AConv,\n\u001b[1;32m     34\u001b[0m     ADown,\n\u001b[1;32m     35\u001b[0m     Attention,\n\u001b[1;32m     36\u001b[0m     BNContrastiveHead,\n\u001b[1;32m     37\u001b[0m     Bottleneck,\n\u001b[1;32m     38\u001b[0m     BottleneckCSP,\n\u001b[1;32m     39\u001b[0m     C2f,\n\u001b[1;32m     40\u001b[0m     C2fAttn,\n\u001b[1;32m     41\u001b[0m     C2fCIB,\n\u001b[1;32m     42\u001b[0m     C2fPSA,\n\u001b[1;32m     43\u001b[0m     C3Ghost,\n\u001b[1;32m     44\u001b[0m     C3k2,\n\u001b[1;32m     45\u001b[0m     C3x,\n\u001b[1;32m     46\u001b[0m     CBFuse,\n\u001b[1;32m     47\u001b[0m     CBLinear,\n\u001b[1;32m     48\u001b[0m     ContrastiveHead,\n\u001b[1;32m     49\u001b[0m     GhostBottleneck,\n\u001b[1;32m     50\u001b[0m     HGBlock,\n\u001b[1;32m     51\u001b[0m     HGStem,\n\u001b[1;32m     52\u001b[0m     ImagePoolingAttn,\n\u001b[1;32m     53\u001b[0m     Proto,\n\u001b[1;32m     54\u001b[0m     RepC3,\n\u001b[1;32m     55\u001b[0m     RepNCSPELAN4,\n\u001b[1;32m     56\u001b[0m     RepVGGDW,\n\u001b[1;32m     57\u001b[0m     ResNetLayer,\n\u001b[1;32m     58\u001b[0m     SCDown,\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     61\u001b[0m     CBAM,\n\u001b[1;32m     62\u001b[0m     ChannelAttention,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     SpatialAttention,\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhead\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OBB, Classify, Detect, Pose, RTDETRDecoder, Segment, WorldDetect, v10Detect\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuse_conv_and_bn\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv, DWConv, GhostConv, LightConv, RepConv, autopad\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerBlock\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/ultralytics/utils/__init__.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Union\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/matplotlib/pyplot.py:55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcycler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolorbar\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/matplotlib/colorbar.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook, collections, cm, colors, contour, ticker\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmartist\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpatches\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/matplotlib/contour.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MouseButton\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Line2D\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/matplotlib/backend_bases.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     _api, backend_tools \u001b[38;5;28;01mas\u001b[39;00m tools, cbook, colors, _docstring, text,\n\u001b[1;32m     51\u001b[0m     _tight_bbox, transforms, widgets, is_interactive, rcParams)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pylab_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Gcf\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToolManager\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/matplotlib/text.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfont_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FontProperties\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FancyArrowPatch, FancyBboxPatch, Rectangle\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtextpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextPath, TextToPath  \u001b[38;5;66;03m# noqa # Logically located here\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     Affine2D, Bbox, BboxBase, BboxTransformTo, IdentityTransform, Transform)\n\u001b[1;32m     23\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/matplotlib/textpath.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _text_helpers, dviread\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfont_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     FontProperties, get_font, fontManager \u001b[38;5;28;01mas\u001b[39;00m _fontManager\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mft2font\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOAD_NO_HINTING, LOAD_TARGET_LIGHT\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/site-packages/matplotlib/_text_helpers.py:13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mft2font\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KERNING_DEFAULT, LOAD_NO_HINTING, FT2Font\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;129;43m@dataclasses\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrozen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mLayoutItem\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mft_object\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mFT2Font\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchar\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/dataclasses.py:1222\u001b[0m, in \u001b[0;36mdataclass.<locals>.wrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m-> 1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_process_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsafe_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mfrozen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweakref_slot\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/dataclasses.py:1020\u001b[0m, in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# Include InitVars and regular fields (so, not ClassVars).  This is\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# initialized here, outside of the \"if init:\" test, because std_init_fields\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# is used with match_args, below.\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m all_init_fields \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m   1018\u001b[0m                    \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_field_type \u001b[38;5;129;01min\u001b[39;00m (_FIELD, _FIELD_INITVAR)]\n\u001b[1;32m   1019\u001b[0m (std_init_fields,\n\u001b[0;32m-> 1020\u001b[0m  kw_only_init_fields) \u001b[38;5;241m=\u001b[39m \u001b[43m_fields_in_init_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_init_fields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;66;03m# Does this class have a post-init function?\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m     has_post_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, _POST_INIT_NAME)\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/dataclasses.py:396\u001b[0m, in \u001b[0;36m_fields_in_init_order\u001b[0;34m(fields)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fields_in_init_order\u001b[39m(fields):\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# Returns the fields as __init__ will output them.  It returns 2 tuples:\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;66;03m# the first for normal args, and the second for keyword args.\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mtuple\u001b[39m(f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39minit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkw_only),\n\u001b[1;32m    397\u001b[0m             \u001b[38;5;28mtuple\u001b[39m(f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39minit \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkw_only)\n\u001b[1;32m    398\u001b[0m             )\n",
      "File \u001b[0;32m~/miniconda3/envs/inzva/lib/python3.11/dataclasses.py:396\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fields_in_init_order\u001b[39m(fields):\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# Returns the fields as __init__ will output them.  It returns 2 tuples:\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;66;03m# the first for normal args, and the second for keyword args.\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mtuple\u001b[39m(f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39minit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkw_only),\n\u001b[1;32m    397\u001b[0m             \u001b[38;5;28mtuple\u001b[39m(f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39minit \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkw_only)\n\u001b[1;32m    398\u001b[0m             )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# pred stuff on yolo\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Images\n",
    "imgs = [\n",
    "    \"https://ultralytics.com/images/zidane.jpg\",\n",
    "    \n",
    "    \"https://lumiere-a.akamaihd.net/v1/images/open-uri20150608-27674-iuiafs_2fd2629d.jpeg\",\n",
    "\n",
    "    \"https://wallpapercave.com/wp/s1o8rpn.jpg\",\n",
    "\n",
    "    \"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.pinimg.com%2Foriginals%2F36%2Fcd%2Feb%2F36cdebcd4fdd7eef3c9d0723cb0a886e.jpg\",\n",
    "\n",
    "    \"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.cctvcamerapros.com%2Fv%2Fimages%2FHD-Security-Cameras%2FHD-TVI-BL2%2Finfrared-HD-TVI-camera-1080p-surveillance.jpg\",\n",
    "    ]\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "\n",
    "# Results\n",
    "results.print()\n",
    "results.save()  # or .show()\n",
    "# results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inzva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
