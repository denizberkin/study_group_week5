{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoping at this point, we are familiar with classification, object detection can be explained as a classification with localization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "![mater](assets/mcqueen_real.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Localization (and over multiple objects)\n",
    "\n",
    "![mcqueen](assets/mcqueen.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Could be used on any kind of task where finding the location of the object(s) are of any use\n",
    "- Anything related to traffic, pedestrians, types of vehicles, drivable roads, landing zones etc.\n",
    "- Anything related to locating a disease over some type of medical imaging (MRI, Ultrasound, CT ...)\n",
    "- When designing automated stores, factories etc. (Like Amazon Go cashierless stores)\n",
    "\n",
    "this could go on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yea yea yea its all good but how does it come to be and how can I learn / use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay we are kind of familiar with a CNN, it acts as a feature extractor, connects to a FCN with number of classes as neurons for output and ta-dah, we have a multi class classifier. \n",
    "\n",
    "![out_neurons](assets/detection_output_neurons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![annotated_out_neurons](assets/annotated_output_neurons.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be penalized with any loss but main logic here is that \n",
    "loss_fn = lambda x, y: (x - y) ** 2\n",
    "prediction = [1] * 8  # P, x, y, w, h, c1, c2, c3\n",
    "label = [1] * 8\n",
    "\n",
    "if prediction[0]:\n",
    "    # calculate loss for only first neuron, we want it to be 0\n",
    "    loss = loss_fn(prediction[0], label[0])\n",
    "else:\n",
    "    # calculate loss over all the other predictions as well\n",
    "    loss = sum([loss_fn(p, l) for p, l in zip(prediction, label)])  # you do not have to use one type of loss function here\n",
    "    # you can use variation of losses which may differ from a bounding box to a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But how do we classify an unknown number of objects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let me explain while expanding on some utility functions that make object detection the way it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding window detection\n",
    "\n",
    "**Sliding Window Detection: Because who needs an exact address when you can just keep \n",
    "looking?**\n",
    "\n",
    "Sliding window detection is like searching for your car in a crowded parking lot - \n",
    "except instead of cars, it's objects in images! This method involves repeatedly \n",
    "applying the same feature detector (or \"window\") to an image at multiple locations \n",
    "and scales. As it slides around, it checks each spot to see if there's a good match.\n",
    "\n",
    "**Example:** Imagine you're looking for your cat hiding behind a couch in a large \n",
    "living room. A sliding window detector would move its feature template over the \n",
    "image, checking every possible location where your cat might be hiding, at different \n",
    "scales and orientations. Eventually, it'll find that sneaky kitty!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/sliding_snail.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Iterable\n",
    "import numpy as np\n",
    "\n",
    "def sliding_window(image: np.array, step_size: int, window_size: Tuple[int, int]) -> Iterable[int, int, np.ndarray]:\n",
    "    H, W = image.shape  # considering image is 2 channels, you should put a check here\n",
    "    for y in range(0, H, step_size):\n",
    "        for x in range(0, W, step_size):\n",
    "            yield (x, y, image[y: y + window_size[1], x: x + window_size[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersection Over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IoU: Because parking your spaceship in the correct spot is crucial**\n",
    "\n",
    "IoU, or Intersection over Union, is a measure of how well two things fit together... \n",
    "literally! It's often used in object detection and computer vision tasks, like \n",
    "identifying cats in images (which can be quite challenging, let's be honest). IoU \n",
    "simply calculates the ratio of the intersection area to the union area between two \n",
    "bounding boxes.\n",
    "\n",
    "**Example:** Imagine you're playing a game where you need to draw a box around a cat \n",
    "in an image. If your box perfectly overlaps with the actual cat, that's a high IoU \n",
    "score (ideally 1). But if your box is way off, only partially covering the cat... not \n",
    "so much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/iou-formula.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/iou-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image_1 reference](https://idiotdeveloper.com/what-is-intersection-over-union-iou/)\n",
    "\n",
    "[image_2 reference](https://www.superannotate.com/blog/intersection-over-union-for-object-detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(pred: Tuple[int, int, int, int], \n",
    "        gt: Tuple[int, int, int, int]) -> float:\n",
    "    \"\"\" in xyxy format, you can write it as xywh format if you'd like \"\"\"\n",
    "    # intersection points\n",
    "    x1 = max(pred[0], gt[0])\n",
    "    y1 = max(pred[0], gt[0])\n",
    "    x2 = max(pred[0], gt[0])\n",
    "    y2 = max(pred[0], gt[0])\n",
    "\n",
    "    # intersection\n",
    "    intersection = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "\n",
    "    # area of boxes\n",
    "    area_pred = (pred[2] - pred[0] + 1) * (pred[3] - pred[1] + 1)\n",
    "    area_gt = (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1)\n",
    "\n",
    "    iou = intersection / float(area_pred + area_gt - intersection)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Max Suppression (NMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMS: Because you can't have multiple cats in one box (or can you?)**\n",
    "\n",
    "NMS, or Non-Maximum Suppression, is like the ultimate party crasher... for bounding \n",
    "boxes! When an object detector spits out multiple bounding boxes around the same \n",
    "thing (like that pesky cat), NMS comes to the rescue. It looks at these boxes and \n",
    "says: \"No way, buddy! There can only be one correct box here.\" Then it selectively \n",
    "suppresses or removes redundant boxes, leaving you with a more accurate and tidier \n",
    "set of predictions.\n",
    "\n",
    "**Example:** Imagine you're playing a game where you need to detect all the cats in \n",
    "an image. If your detector incorrectly identifies multiple cat boxes around the same \n",
    "furry friend, NMS will help eliminate those duplicates, giving you a cleaner and more \n",
    "precise detection result!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/nms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image reference](https://learnopencv.com/weighted-boxes-fusion/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_supression(boxes: list, scores: list, threshold: float = 0.5):\n",
    "    if len(boxes) == 0:\n",
    "        return []  # no prediction to supress\n",
    "    \n",
    "    # it is good to work with np arrays / easier if it is not already that way\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    picked = []\n",
    "    # sorting bbox confidence scores in descending order\n",
    "    indices = np.argsort(scores)[::-1]\n",
    "\n",
    "    while len(indices) > 0:\n",
    "        current = indices[0]\n",
    "        picked.append(current)\n",
    "\n",
    "        # compute iou for all of the rest\n",
    "        remaining = indices[1:]\n",
    "        ious = np.array([iou(boxes[current], boxes[i]) for i in remaining])\n",
    "\n",
    "        indices = remaining[ious < threshold]  # elliminate boxes that computes iou less than the threshold\n",
    "\n",
    "    return boxes[picked]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anchor Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anchor Boxes: Because who needs precision when you can have options?**\n",
    "\n",
    "Anchor boxes are like the buffet of object detection - they offer multiple choices (or \"anchors\") for bounding box predictions. Instead of predicting a single box, an anchor box-based detector proposes a range of possible boxes that might contain an object. This is like saying: \"Hey, I'm not sure if this here or there... but it's probably around here somewhere!\"\n",
    "\n",
    "**Example:** Imagine you're trying to detect all the animals in an image. An anchor box-based detector would propose multiple bounding boxes with different sizes and aspect ratios, covering possible locations and orientations of the animals. The algorithm then adjusts these anchors based on the detected objects' characteristics, like size and shape, to get a more accurate detection result!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchor_boxes(scales: list, aspect_ratios: list, image_size: Tuple[int, int]):\n",
    "    anchor_boxes = []\n",
    "    for scale in scales:\n",
    "        for ratio in aspect_ratios:\n",
    "            width = scale * np.sqrt(ratio)\n",
    "            height = scale / np.sqrt(ratio)\n",
    "            # Create anchor box\n",
    "            anchor_boxes.append([width, height])\n",
    "    return anchor_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding bbox locations based on anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: (1 / (1 + np.exp(-x)))\n",
    "\n",
    "def decode_bbox(predicted_bbox, anchor_bbox, grid_cell, stride):\n",
    "    # converting to absolute coordinates w.r.t. anchor box\n",
    "    bx = (sigmoid(predicted_bbox[0]) + grid_cell[0]) * stride\n",
    "    by = (sigmoid(predicted_bbox[1]) + grid_cell[1]) * stride\n",
    "    bw = anchor_bbox[0] * np.exp(predicted_bbox[2])\n",
    "    bh = anchor_bbox[1] * np.exp(predicted_bbox[3])\n",
    "    \n",
    "    return [bx, by, bw, bh]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side note on how YOLO calculates loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(maybe not the current ones like YOLO7-8-9-10..., can't keep track of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_loss_fn, bce, categorical_ce = None, None, None\n",
    "\n",
    "def yolo_loss(predictions, ground_truth, anchors):\n",
    "    # Split predictions into components\n",
    "    obj_preds = predictions[..., 0]   # objectness\n",
    "    box_preds = predictions[..., 1:5]  # x, y, w, h\n",
    "    class_preds = predictions[..., 6:] # class predictions\n",
    "    \n",
    "    # \"is there\" an object?\n",
    "    obj_loss = bce(obj_preds, ground_truth[..., 0])\n",
    "    \n",
    "    # \"how much\" of the object we have correctly guessed\n",
    "    iou_loss = iou_loss_fn(box_preds, ground_truth[..., 1:5])\n",
    "    \n",
    "    # did we guess \"which\" object it is\n",
    "    class_loss = categorical_ce(class_preds, ground_truth[..., 6:])\n",
    "    \n",
    "    return iou_loss + obj_loss + class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred stuff on yolo\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5x', pretrained=True)\n",
    "\n",
    "# Images\n",
    "imgs = [\n",
    "    # shouting man\n",
    "    \"https://ultralytics.com/images/zidane.jpg\",\n",
    "    \n",
    "    # mcqueen\n",
    "    \"https://lumiere-a.akamaihd.net/v1/images/open-uri20150608-27674-iuiafs_2fd2629d.jpeg\",\n",
    "\n",
    "    # adventure time\n",
    "    \"https://wallpapercave.com/wp/s1o8rpn.jpg\",\n",
    "\n",
    "    # hanging objects\n",
    "    \"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.pinimg.com%2Foriginals%2F36%2Fcd%2Feb%2F36cdebcd4fdd7eef3c9d0723cb0a886e.jpg\",\n",
    "\n",
    "    # surveillance cam\n",
    "    \"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.cctvcamerapros.com%2Fv%2Fimages%2FHD-Security-Cameras%2FHD-TVI-BL2%2Finfrared-HD-TVI-camera-1080p-surveillance.jpg\",\n",
    "    ]  # batch of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "\n",
    "# Results\n",
    "results.print()\n",
    "results.save()  # or .show()\n",
    "# results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inzva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
